{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Model Evaluation - Hateful Meme Detection\n",
    "\n",
    "Comprehensive evaluation of the trained model.\n",
    "\n",
    "**Contents:**\n",
    "1. Load Model\n",
    "2. Validation Metrics\n",
    "3. Confusion Matrix\n",
    "4. ROC & PR Curves\n",
    "5. Threshold Analysis\n",
    "6. Error Analysis\n",
    "7. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "from src.model import create_model\n",
    "from src.dataset import create_dataloaders\n",
    "from src.losses import FocalLoss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = '../models/best_model.pth'\n",
    "DATA_PATH = '../data/hateful_memes'\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "model = create_model(\n",
    "    config=checkpoint.get('model_config'),\n",
    "    device=device\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load data\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "dataloaders = create_dataloaders(DATA_PATH, processor, batch_size=32)\n",
    "val_loader = dataloaders['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "y_true = []\n",
    "y_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = model(pixel_values, input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        \n",
    "        y_true.extend(labels.numpy())\n",
    "        y_prob.extend(probs.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_prob = np.array(y_prob)\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "print(f\"Total samples: {len(y_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_true, y_pred, target_names=['Not Hateful', 'Hateful']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Hateful', 'Hateful'],\n",
    "            yticklabels=['Not Hateful', 'Hateful'])\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Not Hateful', 'Hateful'],\n",
    "            yticklabels=['Not Hateful', 'Hateful'])\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC & PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Find optimal threshold (Youden's J)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds_roc[optimal_idx]\n",
    "\n",
    "axes[0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[0].scatter(fpr[optimal_idx], tpr[optimal_idx], c='red', s=100, \n",
    "                label=f'Optimal Threshold = {optimal_threshold:.3f}')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.3)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_true, y_prob)\n",
    "avg_precision = average_precision_score(y_true, y_prob)\n",
    "\n",
    "axes[1].plot(recall, precision, 'g-', lw=2, label=f'PR (AP = {avg_precision:.4f})')\n",
    "axes[1].fill_between(recall, precision, alpha=0.3, color='green')\n",
    "axes[1].axhline(y=y_true.mean(), color='gray', linestyle='--', label='Baseline')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/roc_pr_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"At optimal: TPR = {tpr[optimal_idx]:.4f}, FPR = {fpr[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "metrics = {'threshold': [], 'f1': [], 'precision': [], 'recall': []}\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (y_prob > t).astype(int)\n",
    "    metrics['threshold'].append(t)\n",
    "    metrics['f1'].append(f1_score(y_true, preds))\n",
    "    metrics['precision'].append(precision_score(y_true, preds))\n",
    "    metrics['recall'].append(recall_score(y_true, preds))\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_metrics['threshold'], df_metrics['f1'], 'b-', label='F1')\n",
    "plt.plot(df_metrics['threshold'], df_metrics['precision'], 'g-', label='Precision')\n",
    "plt.plot(df_metrics['threshold'], df_metrics['recall'], 'r-', label='Recall')\n",
    "plt.axvline(x=optimal_threshold, color='gray', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metrics vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../results/figures/threshold_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Best threshold for F1\n",
    "best_idx = df_metrics['f1'].idxmax()\n",
    "print(f\"\\nBest threshold for F1: {df_metrics.loc[best_idx, 'threshold']:.2f}\")\n",
    "print(f\"F1 at best threshold: {df_metrics.loc[best_idx, 'f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal threshold for predictions\n",
    "y_pred_opt = (y_prob > optimal_threshold).astype(int)\n",
    "\n",
    "# Create results dataframe\n",
    "import json\n",
    "with open(f'{DATA_PATH}/dev.jsonl', 'r') as f:\n",
    "    dev_data = [json.loads(line) for line in f]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'id': [d['id'] for d in dev_data],\n",
    "    'text': [d['text'][:80] for d in dev_data],\n",
    "    'true_label': y_true,\n",
    "    'pred_label': y_pred_opt,\n",
    "    'probability': y_prob,\n",
    "    'correct': y_true == y_pred_opt\n",
    "})\n",
    "\n",
    "# False positives and negatives\n",
    "fp = results_df[(results_df['true_label'] == 0) & (results_df['pred_label'] == 1)]\n",
    "fn = results_df[(results_df['true_label'] == 1) & (results_df['pred_label'] == 0)]\n",
    "\n",
    "print(f\"False Positives: {len(fp)} (Not Hateful predicted as Hateful)\")\n",
    "print(f\"False Negatives: {len(fn)} (Hateful predicted as Not Hateful)\")\n",
    "\n",
    "print(\"\\n--- Top False Positives (High Confidence) ---\")\n",
    "print(fp.nlargest(5, 'probability')[['id', 'probability', 'text']])\n",
    "\n",
    "print(\"\\n--- Top False Negatives (Low Confidence) ---\")\n",
    "print(fn.nsmallest(5, 'probability')[['id', 'probability', 'text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import HatefulMemePredictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = HatefulMemePredictor(\n",
    "    model_path=MODEL_PATH,\n",
    "    device=str(device),\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "# Test on a sample\n",
    "sample = dev_data[0]\n",
    "image_path = f\"{DATA_PATH}/{sample['img']}\"\n",
    "text = sample['text']\n",
    "\n",
    "result = predictor.get_detailed_analysis(image_path, text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INFERENCE DEMO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"True Label: {'Hateful' if sample['label'] == 1 else 'Not Hateful'}\")\n",
    "print(f\"Predicted: {result['label']}\")\n",
    "print(f\"Probability: {result['probability']:.4f}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"Zone: {result['zone']}\")\n",
    "\n",
    "# Show image\n",
    "img = Image.open(image_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Prediction: {result['label']} ({result['probability']:.2%})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "**Key Results:**\n",
    "- Model achieves competitive performance on a challenging multimodal task\n",
    "- Optimal threshold identified through ROC analysis\n",
    "- Error analysis reveals model struggles with subtle/implicit hate\n",
    "\n",
    "**Recommendations:**\n",
    "- Use optimal threshold (not 0.5) for deployment\n",
    "- Implement tiered response system (safe/warning/remove)\n",
    "- Human review for edge cases in warning zone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
