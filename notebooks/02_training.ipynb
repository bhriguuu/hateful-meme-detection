{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Model Training - Hateful Meme Detection\n",
    "\n",
    "Train the CLIP + Cross-Attention Fusion model.\n",
    "\n",
    "**Contents:**\n",
    "1. Setup & Configuration\n",
    "2. Data Loading\n",
    "3. Model Creation\n",
    "4. Training Loop\n",
    "5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "# !pip install torch torchvision transformers albumentations scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "from src.model import HatefulMemeClassifier, create_model\n",
    "from src.dataset import create_dataloaders\n",
    "from src.losses import FocalLoss\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_path': '../data/hateful_memes',\n",
    "    'output_path': '../outputs',\n",
    "    'model_path': '../models',\n",
    "    \n",
    "    # Model\n",
    "    'clip_model': 'openai/clip-vit-base-patch32',\n",
    "    'hidden_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'dropout': 0.3,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'gradient_clip': 1.0,\n",
    "    'patience': 3,\n",
    "    \n",
    "    # Loss\n",
    "    'focal_alpha': 0.6412,  # Based on class distribution\n",
    "    'focal_gamma': 2.0,\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path(CONFIG['output_path']).mkdir(exist_ok=True)\n",
    "Path(CONFIG['model_path']).mkdir(exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and create dataloaders\n",
    "processor = CLIPProcessor.from_pretrained(CONFIG['clip_model'])\n",
    "\n",
    "dataloaders = create_dataloaders(\n",
    "    data_path=CONFIG['data_path'],\n",
    "    processor=processor,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['val']\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(\n",
    "    config={\n",
    "        'clip_model_name': CONFIG['clip_model'],\n",
    "        'hidden_dim': CONFIG['hidden_dim'],\n",
    "        'num_heads': CONFIG['num_heads'],\n",
    "        'dropout': CONFIG['dropout'],\n",
    "        'freeze_clip': True\n",
    "    },\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print parameter count\n",
    "params = model.count_parameters()\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {params['total']:,}\")\n",
    "print(f\"  Trainable: {params['trainable']:,} ({params['trainable_pct']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Loss function\n",
    "criterion = FocalLoss(alpha=CONFIG['focal_alpha'], gamma=CONFIG['focal_gamma'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['learning_rate'],\n",
    "    total_steps=total_steps,\n",
    "    pct_start=CONFIG['warmup_ratio']\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_probs, all_labels = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'auc': roc_auc_score(all_labels, all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': [], 'val_auc': []}\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Log\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_f1'].append(train_metrics['f1'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_auc'].append(val_metrics['auc'])\n",
    "    \n",
    "    print(f\"Train - Loss: {train_metrics['loss']:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_metrics['loss']:.4f}, F1: {val_metrics['f1']:.4f}, AUC: {val_metrics['auc']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        torch.save(model.state_dict(), f\"{CONFIG['model_path']}/best_model.pth\")\n",
    "        print(f\"  >> New best model! F1: {best_f1:.4f}\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTraining complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "\n",
    "# F1\n",
    "axes[1].plot(epochs, history['train_f1'], 'b-', label='Train')\n",
    "axes[1].plot(epochs, history['val_f1'], 'r-', label='Val')\n",
    "axes[1].set_title('F1 Score')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(epochs, history['val_auc'], 'g-')\n",
    "axes[2].set_title('Validation AUC')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_path']}/training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'clip_model_name': CONFIG['clip_model'],\n",
    "        'hidden_dim': CONFIG['hidden_dim'],\n",
    "        'num_heads': CONFIG['num_heads'],\n",
    "        'dropout': CONFIG['dropout'],\n",
    "        'freeze_clip': True\n",
    "    },\n",
    "    'training_config': CONFIG,\n",
    "    'history': history,\n",
    "    'best_f1': best_f1\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, f\"{CONFIG['model_path']}/final_checkpoint.pth\")\n",
    "print(f\"Model saved to {CONFIG['model_path']}/final_checkpoint.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
