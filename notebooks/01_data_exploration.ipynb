{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Data Exploration - Hateful Memes Dataset\n",
    "\n",
    "This notebook explores the Facebook AI Hateful Memes Challenge dataset.\n",
    "\n",
    "**Contents:**\n",
    "1. Dataset Loading\n",
    "2. Class Distribution Analysis\n",
    "3. Text Analysis\n",
    "4. Image Visualization\n",
    "5. Multimodal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = '../data/hateful_memes'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file into list of dicts.\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load all splits\n",
    "train_data = load_jsonl(f'{DATA_PATH}/train.jsonl')\n",
    "dev_data = load_jsonl(f'{DATA_PATH}/dev.jsonl')\n",
    "test_data = load_jsonl(f'{DATA_PATH}/test.jsonl')\n",
    "\n",
    "print(f\"Train: {len(train_data):,} samples\")\n",
    "print(f\"Dev: {len(dev_data):,} samples\")\n",
    "print(f\"Test: {len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "dev_df = pd.DataFrame(dev_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"Sample entry:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = train_df['label'].value_counts()\n",
    "axes[0].pie(train_counts, labels=['Not Hateful', 'Hateful'], \n",
    "            autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Training Set Distribution')\n",
    "\n",
    "# Dev set\n",
    "dev_counts = dev_df['label'].value_counts()\n",
    "axes[1].pie(dev_counts, labels=['Not Hateful', 'Hateful'],\n",
    "            autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Dev Set Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass Imbalance Ratio: {train_counts[0]/train_counts[1]:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length statistics\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Text length distribution\n",
    "for label in [0, 1]:\n",
    "    subset = train_df[train_df['label'] == label]\n",
    "    label_name = 'Hateful' if label == 1 else 'Not Hateful'\n",
    "    axes[0].hist(subset['text_length'], alpha=0.6, bins=30, label=label_name)\n",
    "\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Text Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "train_df.boxplot(column='word_count', by='label', ax=axes[1])\n",
    "axes[1].set_xlabel('Label (0=Not Hateful, 1=Hateful)')\n",
    "axes[1].set_ylabel('Word Count')\n",
    "axes[1].set_title('Word Count by Class')\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds\n",
    "hateful_text = ' '.join(train_df[train_df['label'] == 1]['text'].str.lower())\n",
    "not_hateful_text = ' '.join(train_df[train_df['label'] == 0]['text'].str.lower())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "wc_hateful = WordCloud(width=800, height=400, background_color='white',\n",
    "                       colormap='Reds').generate(hateful_text)\n",
    "axes[0].imshow(wc_hateful, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Hateful Memes - Word Cloud', fontsize=14, color='red')\n",
    "\n",
    "wc_not_hateful = WordCloud(width=800, height=400, background_color='white',\n",
    "                           colormap='Greens').generate(not_hateful_text)\n",
    "axes[1].imshow(wc_not_hateful, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Not Hateful Memes - Word Cloud', fontsize=14, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample memes\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get samples\n",
    "hateful = train_df[train_df['label'] == 1].sample(4, random_state=42)\n",
    "not_hateful = train_df[train_df['label'] == 0].sample(4, random_state=42)\n",
    "\n",
    "samples = pd.concat([not_hateful, hateful])\n",
    "titles = ['NOT HATEFUL'] * 4 + ['HATEFUL'] * 4\n",
    "colors = ['green'] * 4 + ['red'] * 4\n",
    "\n",
    "for ax, (_, row), title, color in zip(axes, samples.iterrows(), titles, colors):\n",
    "    img_path = f\"{DATA_PATH}/{row['img']}\"\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        text = row['text'][:40] + '...' if len(row['text']) > 40 else row['text']\n",
    "        ax.set_title(f\"{title}\\n\\\"{text}\\\"\", fontsize=9, color=color)\n",
    "    except:\n",
    "        ax.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Insights\n",
    "\n",
    "**Findings:**\n",
    "1. Class imbalance exists (64% not hateful, 36% hateful)\n",
    "2. Hateful memes tend to have slightly longer text\n",
    "3. The challenge requires understanding image-text interaction\n",
    "4. Similar words appear in both classes - context matters\n",
    "\n",
    "**Implications for modeling:**\n",
    "- Use Focal Loss or class weighting for imbalance\n",
    "- Need multimodal fusion, not just separate encoders\n",
    "- Cross-attention can capture image-text relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
